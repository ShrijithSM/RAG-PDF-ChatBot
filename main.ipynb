{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef1125e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 98 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PDF from: note.pdf\n",
      "Total characters loaded: 3490\n",
      "\n",
      "First 500 characters of the loaded PDF text:\n",
      " \n",
      "SHRIJITH  S  MENON  shrijithsmenon@gmail.com\n",
      " \n",
      "|\n",
      " +91-8891874928  |  linkedin.com/in/shrijithsm  |  github.com/shrijithsm  |  shrijithsm.tech  \n",
      " \n",
      "                                     \n",
      " \n",
      "Skills \n",
      " \n",
      "Python  |  Problem  Solving  |  OOPs  |  Machine  Learning  |  NumPy  |  Pandas  |  MongoDB  |  MySQL|  SQLite  |  HTML/CSS  |  \n",
      "Java\n",
      " \n",
      "|\n",
      " \n",
      "Git\n",
      " \n",
      "|\n",
      " \n",
      "Linux\n",
      " \n",
      "  \n",
      "Education \n",
      " \n",
      "Jain  (Deemed-to-be-university),  Bengaluru,  India       \n",
      "   \n",
      " \n",
      " \n",
      "                        2023  -  2027 ●  B.Tech  Computer  Sc\n"
     ]
    }
   ],
   "source": [
    "# Cell 2\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Loads text content from a PDF file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        print(f\"Loaded PDF from: {file_path}\")\n",
    "        print(f\"Total characters loaded: {len(text)}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "# Replace 'sample.pdf' with the actual path to your PDF file\n",
    "pdf_path = 'note.pdf' # Make sure you have a sample.pdf in your directory or provide full path\n",
    "pdf_text = load_pdf(pdf_path)\n",
    "\n",
    "if pdf_text:\n",
    "    print(\"\\nFirst 500 characters of the loaded PDF text:\")\n",
    "    print(pdf_text[:500])\n",
    "else:\n",
    "    print(\"PDF loading failed. Please check the file path and ensure the PDF is valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e23e49fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text split into 5 chunks.\n",
      "Example chunk (first 200 chars):\n",
      "SHRIJITH  S  MENON  shrijithsmenon@gmail.com\n",
      " \n",
      "|\n",
      " +91-8891874928  |  linkedin.com/in/shrijithsm  |  github.com/shrijithsm  |  shrijithsm.tech  \n",
      " \n",
      "                                     \n",
      " \n",
      "Skills \n",
      " \n",
      "Pyth\n"
     ]
    }
   ],
   "source": [
    "# Cell 3\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Splits a given text into smaller chunks.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.create_documents([text])\n",
    "    print(f\"\\nText split into {len(chunks)} chunks.\")\n",
    "    print(f\"Example chunk (first 200 chars):\")\n",
    "    print(chunks[0].page_content[:200])\n",
    "    return chunks\n",
    "\n",
    "if pdf_text:\n",
    "    text_chunks = split_text_into_chunks(pdf_text)\n",
    "else:\n",
    "    text_chunks = []\n",
    "    print(\"Cannot split text because PDF loading failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd09798a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "d:\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Shrijith\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer model 'all-MiniLM-L6-v2' loaded successfully.\n",
      "\n",
      "Generating embeddings for text chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5 embeddings, each with dimension 384.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "def get_embeddings_model():\n",
    "    \"\"\"\n",
    "    Loads a pre-trained Sentence Transformer model for embeddings.\n",
    "    \"\"\"\n",
    "    # Using 'all-MiniLM-L6-v2' as it's good for semantic similarity and relatively small.\n",
    "    # For better performance, consider 'all-MiniLM-L12-v2' or 'multi-qa-mpnet-base-dot-v1'.\n",
    "    model_name = 'all-MiniLM-L6-v2'\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name)\n",
    "        print(f\"SentenceTransformer model '{model_name}' loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading SentenceTransformer model: {e}\")\n",
    "        print(\"Please ensure you have an active internet connection to download the model if it's not cached.\")\n",
    "        return None\n",
    "\n",
    "embedding_model = get_embeddings_model()\n",
    "\n",
    "if embedding_model and text_chunks:\n",
    "    print(\"\\nGenerating embeddings for text chunks...\")\n",
    "    # Extract just the page_content from the Document objects\n",
    "    chunk_contents = [chunk.page_content for chunk in text_chunks]\n",
    "    chunk_embeddings = embedding_model.encode(chunk_contents, show_progress_bar=True)\n",
    "    print(f\"Generated {len(chunk_embeddings)} embeddings, each with dimension {chunk_embeddings.shape[1]}.\")\n",
    "else:\n",
    "    print(\"Cannot generate embeddings: Embedding model or text chunks not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c873e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index or texts not found. Please create them first. Missing: pdf_rag_faiss_index.bin or pdf_rag_texts.npy\n",
      "FAISS index created with 5 vectors.\n",
      "FAISS index saved to pdf_rag_faiss_index.bin\n",
      "Associated texts saved to pdf_rag_texts.npy\n"
     ]
    }
   ],
   "source": [
    "# Cell 5\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def create_faiss_index(embeddings, texts):\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from embeddings and stores the corresponding texts.\n",
    "    \"\"\"\n",
    "    if embeddings is None or len(embeddings) == 0:\n",
    "        print(\"No embeddings to create FAISS index.\")\n",
    "        return None, None\n",
    "\n",
    "    # Ensure embeddings are float32 as required by FAISS\n",
    "    embeddings = np.array(embeddings).astype('float32')\n",
    "    dimension = embeddings.shape[1]\n",
    "\n",
    "    # Create an IndexFlatL2 index (L2 for Euclidean distance, suitable for many embedding types)\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    print(f\"FAISS index created with {index.ntotal} vectors.\")\n",
    "    return index, texts # Store original texts alongside the index\n",
    "\n",
    "def save_faiss_index(index, texts, index_name=\"pdf_rag_faiss_index.bin\", texts_name=\"pdf_rag_texts.npy\"):\n",
    "    \"\"\"\n",
    "    Saves the FAISS index and associated texts to disk.\n",
    "    \"\"\"\n",
    "    if index is None or texts is None:\n",
    "        print(\"Nothing to save: FAISS index or texts are missing.\")\n",
    "        return\n",
    "\n",
    "    faiss.write_index(index, index_name)\n",
    "    np.save(texts_name, texts)\n",
    "    print(f\"FAISS index saved to {index_name}\")\n",
    "    print(f\"Associated texts saved to {texts_name}\")\n",
    "\n",
    "def load_faiss_index(index_name=\"pdf_rag_faiss_index.bin\", texts_name=\"pdf_rag_texts.npy\"):\n",
    "    \"\"\"\n",
    "    Loads the FAISS index and associated texts from disk.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(index_name) or not os.path.exists(texts_name):\n",
    "        print(f\"FAISS index or texts not found. Please create them first. Missing: {index_name} or {texts_name}\")\n",
    "        return None, None\n",
    "    try:\n",
    "        index = faiss.read_index(index_name)\n",
    "        texts = np.load(texts_name, allow_pickle=True).tolist() # Convert back to list of Documents\n",
    "        print(f\"FAISS index loaded from {index_name}\")\n",
    "        print(f\"Associated texts loaded from {texts_name}\")\n",
    "        return index, texts\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FAISS index or texts: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Try to load existing index, otherwise create and save\n",
    "faiss_index, indexed_texts = load_faiss_index()\n",
    "\n",
    "if faiss_index is None:\n",
    "    if embedding_model and text_chunks and chunk_embeddings is not None:\n",
    "        faiss_index, indexed_texts = create_faiss_index(chunk_embeddings, text_chunks)\n",
    "        if faiss_index: # Only save if creation was successful\n",
    "            save_faiss_index(faiss_index, indexed_texts)\n",
    "    else:\n",
    "        print(\"Cannot create FAISS index: Missing embedding model, text chunks, or chunk embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22cb46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "def retrieve_chunks(query, embedding_model, faiss_index, indexed_texts, k=5):\n",
    "    \"\"\"\n",
    "    Retrieves the top-k most similar text chunks to a given query.\n",
    "    \"\"\"\n",
    "    if embedding_model is None or faiss_index is None or indexed_texts is None:\n",
    "        print(\"Cannot retrieve chunks: Missing embedding model, FAISS index, or indexed texts.\")\n",
    "        return []\n",
    "\n",
    "    query_embedding = embedding_model.encode([query]).astype('float32')\n",
    "\n",
    "    # D, I are distances and indices\n",
    "    distances, indices = faiss_index.search(query_embedding, k)\n",
    "\n",
    "    retrieved_chunks = [indexed_texts[i].page_content for i in indices[0]]\n",
    "    print(f\"\\nRetrieved {len(retrieved_chunks)} relevant chunks for the query.\")\n",
    "    # print(\"Retrieved chunks preview:\")\n",
    "    # for i, chunk in enumerate(retrieved_chunks):\n",
    "    #     print(f\"Chunk {i+1}:\\n{chunk[:200]}...\\n\") # Print first 200 chars of each chunk\n",
    "    return retrieved_chunks\n",
    "\n",
    "# Example Retrieval:\n",
    "# if embedding_model and faiss_index and indexed_texts:\n",
    "#     sample_query = \"What is Retrieval-Augmented Generation?\"\n",
    "#     retrieved_content = retrieve_chunks(sample_query, embedding_model, faiss_index, indexed_texts, k=3)\n",
    "#     if retrieved_content:\n",
    "#         print(\"\\nCombined retrieved content for LLM:\")\n",
    "#         print(\"\\n--- NEW CHUNK ---\\n\".join(retrieved_content))\n",
    "# else:\n",
    "#     print(\"Skipping example retrieval: Prerequisites not met.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac29d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading local LLM: google/flan-t5-small...\n",
      "Using device: CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local LLM loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Initialize the LLM\n",
    "llm_model_name = \"google/flan-t5-small\" # A relatively small model for local inference\n",
    "# You might try 'google/flan-t5-base' or 'MBZUAI/LaMini-Flan-T5-77M' for slightly better performance if 'small' is too weak.\n",
    "# For more advanced usage, consider quantized models or local models via ctransformers if performance is an issue.\n",
    "\n",
    "try:\n",
    "    print(f\"\\nLoading local LLM: {llm_model_name}...\")\n",
    "    # Using 'cuda' if GPU is available, otherwise 'cpu'\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "    # For Flan-T5, AutoModelForSeq2SeqLM is appropriate\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name,\n",
    "                                                  torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "\n",
    "    # Use pipeline for ease of generation\n",
    "    generator = pipeline(\n",
    "        \"text2text-generation\", # For Flan-T5 models, this is common\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        max_new_tokens=200, # Max length of the generated answer\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    print(\"Local LLM loaded successfully.\")\n",
    "except Exception as e:\n",
    "    generator = None\n",
    "    print(f\"Error loading local LLM: {e}\")\n",
    "    print(\"Ensure you have enough memory (RAM/VRAM) and a stable internet connection for initial download.\")\n",
    "    print(\"If you encounter OOM errors, try a smaller model or run on CPU if GPU memory is insufficient.\")\n",
    "\n",
    "def generate_answer(query, retrieved_chunks, llm_generator):\n",
    "    \"\"\"\n",
    "    Generates an answer using the local LLM, conditioned on retrieved chunks.\n",
    "    \"\"\"\n",
    "    if llm_generator is None:\n",
    "        print(\"LLM generator not available. Cannot generate answer.\")\n",
    "        return \"Error: Language model not loaded.\"\n",
    "\n",
    "    context = \"\\n\".join(retrieved_chunks)\n",
    "\n",
    "    # Construct the prompt for the LLM\n",
    "    # The prompt engineering here is crucial. Experiment with different formats.\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    # print(f\"\\n--- LLM Prompt --- \\n{prompt[:1000]}...\\n--- End Prompt ---\\n\") # Print a snippet of the prompt\n",
    "\n",
    "    try:\n",
    "        response = llm_generator(prompt)\n",
    "        # The output format depends on the pipeline and model. For text2text-generation, it's usually a list of dicts.\n",
    "        answer = response[0]['generated_text']\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer with LLM: {e}\")\n",
    "        return \"An error occurred while generating the answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59b07b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Chatbot Test ---\n",
      "\n",
      "User Query: what is the users name\n",
      "\n",
      "Retrieved 3 relevant chunks for the query.\n",
      "\n",
      "Chatbot: Jupyter Notebooks\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Query: hello\n",
      "\n",
      "Retrieved 3 relevant chunks for the query.\n",
      "\n",
      "Chatbot: i m in this class i am in this class\n",
      "--------------------------------------------------\n",
      "\n",
      "User Query: what are the skills of the person\n",
      "\n",
      "Retrieved 3 relevant chunks for the query.\n",
      "\n",
      "Chatbot: demonstrating strong problem-solving and teamwork skills\n",
      "--------------------------------------------------\n",
      "\n",
      "User Query: what is the email id\n",
      "\n",
      "Retrieved 3 relevant chunks for the query.\n",
      "\n",
      "Chatbot: cnte\n",
      "--------------------------------------------------\n",
      "\n",
      "User Query: what is the name of the user\n",
      "\n",
      "Retrieved 3 relevant chunks for the query.\n",
      "\n",
      "Chatbot: shrijithsmenon@gmail.com\n",
      "--------------------------------------------------\n",
      "\n",
      "User Query: what is the phone number\n",
      "\n",
      "Retrieved 3 relevant chunks for the query.\n",
      "\n",
      "Chatbot: 437\n",
      "--------------------------------------------------\n",
      "\n",
      "User Query: \n",
      "\n",
      "Retrieved 3 relevant chunks for the query.\n",
      "\n",
      "Chatbot: What were the best features of the product?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 8\n",
    "def chat_with_pdf(query, embedding_model, faiss_index, indexed_texts, llm_generator, num_retrieved_chunks=3):\n",
    "    \"\"\"\n",
    "    Main function to answer a query about the PDF.\n",
    "    \"\"\"\n",
    "    if not (embedding_model and faiss_index and indexed_texts and llm_generator):\n",
    "        return \"Chatbot not fully initialized. Please ensure all components are loaded.\"\n",
    "\n",
    "    print(f\"\\nUser Query: {query}\")\n",
    "\n",
    "    # 1. Retrieve relevant chunks\n",
    "    retrieved_content = retrieve_chunks(query, embedding_model, faiss_index, indexed_texts, k=num_retrieved_chunks)\n",
    "\n",
    "    if not retrieved_content:\n",
    "        return \"Could not retrieve relevant information from the PDF for your query.\"\n",
    "\n",
    "    # 2. Generate answer using the LLM\n",
    "    answer = generate_answer(query, retrieved_content, llm_generator)\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Test the full chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure all components are loaded from previous cells\n",
    "    if not (embedding_model and faiss_index and indexed_texts and generator):\n",
    "        print(\"One or more required components (embedding_model, faiss_index, indexed_texts, generator) are missing or failed to load. Please re-run previous cells.\")\n",
    "    else:\n",
    "        print(\"\\n--- Starting Chatbot Test ---\")\n",
    "        while True:\n",
    "            user_question = input(\"Ask a question about the PDF (type 'quit' to exit): \")\n",
    "            if user_question.lower() == 'quit':\n",
    "                break\n",
    "            response = chat_with_pdf(user_question, embedding_model, faiss_index, indexed_texts, generator)\n",
    "            print(f\"\\nChatbot: {response}\")\n",
    "            print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
